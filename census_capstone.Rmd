---
title: "Prediction of Monthly Sales in the US Census Data"
author: "Irene Siemen"
date: "6/10/2020"
output: pdf_document
---

```{r setup, echo=FALSE, message = FALSE}
#Install required packages
if (!require(dplyr)) install.packages('dplyr')
if (!require(ggplot2)) install.packages('ggplot2')
if (!require(caret)) install.packages('caret')
if (!require(stringr)) install.packages('stringr')
if (!require(tidyverse)) install.packages('tidyverse')
if (!require(tidyr)) install.packages('tidyr')
if (!require(FNN)) install.packages('FNN')
if (!require(purrr)) install.packages('purrr')

library(dplyr)
library(ggplot2)
library(stringr)
library(tidyverse)
library(caret)
library(tidyr)
library(FNN)
library(purrr)

options(digits = 3)
```

## Introduction

The US Census gathers monthly data of the country's total sales and inventory for manufacturers, merchant wholesalers, and retailers. This project focuses on analyzing the dataset and then creating predictions for the total business monthly sales. The dataset contains records dating back to January 1992. Each month and year contains values for monthly sales, inventory, percent change of sales, percent change of inventory, and the inventory to sales ratio. Throughout the data all of the dollar values are reported in millions. The total business monthly sales predictions were generated by creating and training a machine learning algorithm using a subset of the data. Then, the model calculated predictions on the validation dataset. The success of the machine learning algorithm was evaluated using the root mean square error (RMSE).

The first step was to complete an exploratory data analysis (EDA) on the training data set to review the data. The EDA included reviewing the characteristics of the variables, checking for NaNs, performing filters and counts, and graphing the relationships between variables. The EDA revealed trends in the data, such as increasing sales values over time, and these trends were used as factors in the machine learning models. Once the data was put into a tidy format, then a linear model was created to determine predict the monthly sales based on the month and year. Next, a K-Nearest Neighbors (KNN) model was created on the train dataset to calculate more accurate predictions. The fitted KNN model was used to create predictions on the test data and returned a desirable RMSE value. Finally, the KNN model created predictions for the validation dataset and calculated the final RMSE value.

## Methods and Analysis

#### Downloading the data

The US Census Manufacturing and Trade Inventory and Sales data can be downloaded directly from their website at: https://www.census.gov/econ/currentdata/datasets/MTIS-mf.zip

```{r url}
#Download the dataset
dl <- tempfile()
download.file("https://www.census.gov/econ/currentdata/datasets/MTIS-mf.zip", dl)

#Access the data files:
original <- read.csv(unzip(dl, "MTIS-mf.csv"),
                     sep = ",", 
                     col.names = c('idx','code','desc','col4','col5','col6','col7'), 
                     fill = TRUE, 
                     blank.lines.skip = TRUE, 
                     nrows =  19400,
                     header = FALSE,
                     stringsAsFactors = FALSE
                  )
```

#### Preparing the data

The original data was stored in a .csv file. The file had 8 tables listed on one sheet. The first 7 tables contained codes and descriptions. The last table contained the actual data and referenced codes from the first 7 tables. The data needed to be transformed into tidy data. The first step was to store each table as a separate data frames. 

```{r data_extraction, message= FALSE, echo=FALSE}
categories<-original[3:6,1:3]
colnames(categories)<-c("cat_idx","cat_code","cat_desc")
categories<-categories%>%mutate(cat_idx = as.numeric(cat_idx))

datatypes<-original[9:13,1:4]
colnames(datatypes)<-c("dt_idx","dt_code","dt_desc","dt_unit")
datatypes<-datatypes%>%mutate(dt_idx = as.numeric(dt_idx))

errortypes<-original[16:20,1:4]
colnames(errortypes)<-c("et_idx","et_code","et_desc","et_unit")
errortypes<-errortypes%>%mutate(et_idx = as.numeric(et_idx))

timeperiods<-original[26:364,1:2]
colnames(timeperiods)<-c("per_idx","period")
timeperiods<-timeperiods%>%mutate(period = as.character(period),
                                  per_idx = as.numeric(per_idx))

timeperiods<-timeperiods%>%
  mutate(month = str_extract(period,c("Jan",'Feb','Mar','Apr','May',"Jun",'Jul','Aug','Sep','Oct','Nov','Dec')),
         year = as.integer(str_extract(period,"\\d{4}")))

#Extract the actual data table.
data<-original[371:19400,]
colnames(data)<-c("per_idx","cat_idx","dt_idx","et_idx","geo_idx","is_adj","value")
```

The next step was to check for NaN values and to assign the correct data type for each columns. Then, the data frames containing the codes and description were connected to the primary data using a left join.

```{r NaN}
#Assign the correct data types to each column
data<-data%>%mutate(per_idx = as.numeric(per_idx),
                    cat_idx = as.numeric(cat_idx),
                    dt_idx = as.numeric(dt_idx),
                    et_idx = as.numeric(et_idx),
                    geo_idx = as.numeric(geo_idx),
                    is_adj = as.numeric(is_adj),
                    value = as.numeric(value))

#Remove NaN values
data<-data[complete.cases(data), ]

#Use a left join to combine the codes tables with the actual data table.
data<-left_join(data,timeperiods, by = "per_idx")
data<-left_join(data,categories, by = "cat_idx")
data<-left_join(data, datatypes, by = "dt_idx")
data<-left_join(data, errortypes, by = "et_idx")
```

Every time period originally had multiple rows because there was a separate row for each data type (sales, inventory, percent changes) and category (total business, manufacturing, merchant wholesalers, and retail). Tidy data should have one row for each time period with separate columns for the different data and category values. Additional columns were added for the pervious period's monthly sales and percent changes.

```{r tidy}
#Convert to tidy data so that each time period displays the all of the data types and 
#category values in one row
data<-data%>%
  unite(dt_desc,cat_desc,col = "dt_cat", sep = ": ")%>%
  unite(dt_code,cat_code, col = "dt_cat_code" , sep = "_")%>%
  select(per_idx, period, month, year, dt_cat_code,value, is_adj, et_idx)%>%
  spread(dt_cat_code,value)%>%
  filter(is_adj == 0, et_idx == 0)%>%
  select(-c('NA_RETAIL','NA_TOTBUS','NA_WHLSLR'))

#Add columns for the previous period's Total Business monthly sales and percent change
data<- data%>%
  mutate(PREV_SM_TOTBUS = lag(SM_TOTBUS),
         PREV_SM_MNFCTR = lag(SM_MNFCTR),
         PREV_SM_RETAIL = lag(SM_RETAIL),
         PREV_SM_WHLSLR = lag(SM_WHLSLR),
         PREV_MPCSM_TOTBUS = lag(MPCSM_TOTBUS),
         PREV_MPCSM_MNFCTR = lag(MPCSM_MNFCTR),
         PREV_MPCSM_RETAIL = lag(MPCSM_RETAIL),
         PREV_MPCSM_WHLSLR = lag(MPCSM_WHLSLR))
  
head(data)
```

#### Partitioning the data

This was a relatively small dataset because there was only one row per month dating back to January 1992. The data was partitioned into 3 sets: validation, test and train. This allowed the machine learning algorithm to be developed on the training data and then evaluated on the test and validation data. If the data was not partitioned, then the results would have been over-confident. The validation data set contained 10% of the data. The remaining 90% of the data was then partitioned again, so that test data contained 10% and the training data contained 90% of the remaining data. A 90-10 split was selected instead of an 80-20 or 50-50 because the dataset was relatively small so it was important to train on as much data as possible. 

```{r partition}
#Partition out the train and validation data set. 
set.seed(1)
temp_index <- createDataPartition(y = data$per_idx, times = 1, p = 0.1, list = FALSE)
train <- data[-temp_index,]
validation <- data[temp_index,]
dim(validation)
dim(train)

#Partition out the test data set from the training data set
temp_index <- createDataPartition(y = train$per_idx, times = 1, p = 0.1, list = FALSE)
train <- train[-temp_index,]
test <- train[temp_index,]
dim(test)
dim(train)

#Remove NaNs
train<-train[complete.cases(train), ]
test<-test[complete.cases(test), ]
validation<-validation[complete.cases(validation), ]
```

#### Exploratory Data Analysis

The first step of the EDA was to review summaries of the training data.

```{r summary_data, results='hide'}
str(train)
summary(train)

min_sales<-train%>%summarize(min = min(SM_TOTBUS))%>%pull(min)
max_sales<-train%>%summarize(max = max(SM_TOTBUS))%>%pull(max)
avg_sales<-train%>%summarize(avg = mean(SM_TOTBUS))%>%pull(avg)
```

The next step was to create various plots and look for any patterns or trends in the data. A timeseries plot of monthly sales was created first. The plot has separate line for each category. Throughout the data all of the dollar values are reported in millions.

```{r plot1, echo=FALSE}
train%>%
  select(period, month, year, SM_MNFCTR, SM_RETAIL, SM_WHLSLR,SM_TOTBUS)%>%
  gather(category, value, c('SM_MNFCTR', 'SM_RETAIL', 'SM_WHLSLR','SM_TOTBUS'))%>%
  filter(!is.na(value))%>%
  mutate(cat_desc = case_when(category == 'SM_MNFCTR' ~ 'Manufacturer', 
                              category == 'SM_RETAIL' ~ 'Retail', 
                              category == 'SM_WHLSLR' ~ 'Merchant Wholesalers',
                              category == 'SM_TOTBUS' ~ 'Total Business'))%>%
  group_by(year, cat_desc)%>%
  summarize(total_value = sum(value))%>%
  ggplot(aes(year,total_value, color = cat_desc))+
  geom_line()+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ylab("Total Annual Value in Millions")+
  xlab("Year")+
  ggtitle("Monthly Sales per Year for Each Category")+
  labs(color = "Category")
```
\newpage
A similar timeseries plot was created displaying inventory values instead of sales.

```{r plot2, echo=FALSE}
train%>%
  select(period, month, year, IM_MNFCTR, IM_RETAIL, IM_WHLSLR,IM_TOTBUS)%>%
  gather(category, value, c('IM_MNFCTR', 'IM_RETAIL', 'IM_WHLSLR','IM_TOTBUS'))%>%
  filter(!is.na(value))%>%
  mutate(cat_desc = case_when(category == 'IM_MNFCTR' ~ 'Manufacturer', 
                              category == 'IM_RETAIL' ~ 'Retail', 
                              category == 'IM_WHLSLR' ~ 'Merchant Wholesalers',
                              category == 'IM_TOTBUS' ~ 'Total Business'))%>%
  group_by(year, cat_desc)%>%
  summarize(total_value = sum(value))%>%
  ggplot(aes(year,total_value, color = cat_desc))+
  geom_line()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ylab("Total Annual Value in Millions")+
  xlab("Year")+
  ggtitle("Inventory per Year for Each Category")+
  labs(color = "Category")
```
\newpage
A third timeseries was created to compare the inventory and sales values for the total business category.

```{r plot3, echo=FALSE}
train%>%
  select(period, month, year, SM_TOTBUS,IM_TOTBUS)%>%
  gather(category, value, c('SM_TOTBUS','IM_TOTBUS'))%>%
  filter(!is.na(value))%>%
  mutate(cat_desc = case_when(category == 'SM_TOTBUS' ~ 'Sales',
                              category == 'IM_TOTBUS' ~ 'Inventory'))%>%
  group_by(year, cat_desc)%>%
  summarize(total_value = sum(value))%>%
  ggplot(aes(year,total_value, color = cat_desc))+
  geom_line()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ylab("Total Annual Value in Millions")+
  xlab("Year")+
  ggtitle("Total Value per Year for Sales and Inventory")+
  labs(color = "Type")
```
\newpage
The data also contains the inventory to sales ratio. This was included in a timeseries and faceted by category because the chart was too messy when all of the categories were on the same plot.

```{r plot4, echo=FALSE}
train%>%
  select(period, month, year, IR_MNFCTR, IR_RETAIL, IR_WHLSLR,IR_TOTBUS)%>%
  gather(category, value, c('IR_MNFCTR', 'IR_RETAIL', 'IR_WHLSLR','IR_TOTBUS'))%>%
  filter(!is.na(value))%>%
  mutate(cat_desc = case_when(category == 'IR_MNFCTR' ~ 'Manufacturer', 
                              category == 'IR_RETAIL' ~ 'Retail', 
                              category == 'IR_WHLSLR' ~ 'Merchant Wholesalers',
                              category == 'IR_TOTBUS' ~ 'Total Business'))%>%
  group_by(year, cat_desc)%>%
  summarize(total_value = sum(value))%>%
  ggplot(aes(year,total_value, color = cat_desc))+
  geom_line()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1),legend.position = "none")+
  ylab("Ratio")+
  xlab("Year")+
  ggtitle("Adjusted Inventory to Sales Ratio per Year for Each Category")+
  facet_wrap(.~cat_desc,nrow=2,ncol=2)
```
\newpage
The timeseries plots revealed an overall upward trend in both sales and inventory as time progressed. It also showed how drastically low the sales and inventory have dropped in 2020. However, the drop was a bit misleading because it only contains data for the first 3 months in 2020 and the previous years contain data for all 12 months. 

The next step in the EDA explored trends during previous declines. The sales data showed temporary declines in 2004 and 2015, and a prolonged decline between 2008-2010. A timeseries was created to the compared the 2004 and 2015 data.

```{r plot5, echo=FALSE}
train%>%
  select(period, month, year, SM_MNFCTR, SM_RETAIL, SM_WHLSLR)%>%
  gather(category, value, c('SM_MNFCTR', 'SM_RETAIL', 'SM_WHLSLR'))%>%
  filter(!is.na(value),
         year %in% c(2004,2005,2015,2016))%>%
  mutate(cat_desc = case_when(category == 'SM_MNFCTR' ~ 'Manufacturer', 
                              category == 'SM_RETAIL' ~ 'Retail', 
                              category == 'SM_WHLSLR' ~ 'Merchant Wholesalers'),
         year_group =case_when (year %in% 2004:2005 ~ '2004 - 2005',
                                year %in% 2015:2016 ~ '2015 - 2016'))%>%
  mutate(period_date = as.Date(paste('01', period), format = "%d %b%Y"))%>%
  select(cat_desc,year,value, period_date, year_group)%>%
  group_by(period_date,cat_desc, year_group)%>%
  summarize(total_value = sum(value))%>%
  ggplot(aes(period_date,total_value, color = cat_desc))+
  geom_line()+
  theme_bw()+
  scale_color_manual(values = c("#035789", "#890357", "#578903"))+
  ylab("Total Value in Millions")+
  xlab("Date")+
  ggtitle("Adjusted Monthly Sales in Years of Decline")+
  labs(color = "Category")+
  facet_grid(.~year_group, scales = "free")+  
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
\newpage
A separate timeseries was created just for the 2008 - 2010 data. In both plots, it appeared that retail sales increased after a period of decline quicker that manufacturers and merchant wholesalers.

```{r plot6, echo=FALSE}
train%>%
  select(period, month, year, SM_MNFCTR, SM_RETAIL, SM_WHLSLR)%>%
  gather(category, value, c('SM_MNFCTR', 'SM_RETAIL', 'SM_WHLSLR'))%>%
  filter(!is.na(value),
         year %in% 2008:2010)%>%
  mutate(cat_desc = case_when(category == 'SM_MNFCTR' ~ 'Manufacturer', 
                              category == 'SM_RETAIL' ~ 'Retail', 
                              category == 'SM_WHLSLR' ~ 'Merchant Wholesalers'))%>%
  mutate(period_date = as.Date(paste('01', period), format = "%d %b%Y"))%>%
  select(cat_desc,year,value, period_date)%>%
  group_by(period_date,cat_desc)%>%
  summarize(total_value = sum(value))%>%
  ggplot(aes(period_date,total_value, color = cat_desc))+
  geom_line()+
  theme_bw()+
  scale_color_manual(values = c("#035789", "#890357", "#578903"))+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ylab("Total Value in Millions")+
  xlab("Date")+
  ggtitle("Adjusted Sales 2008-2010 for Each Category")+
  labs(color = "Category")
```
\newpage
Next, a plot was created to see if seasonality was a factor of the monthly sales. The plot showed that there tended to be an increase in March, decrease in July, and increase in August.

```{r plot7, echo=FALSE}
train%>%filter(!is.na(SM_TOTBUS))%>%
    mutate(month_number = case_when(month=='Jan' ~ 1,
                                    month=='Feb' ~ 2,
                                    month=='Mar' ~ 3,
                                    month=='Apr' ~ 4,
                                    month=='May' ~ 5,
                                    month=='Jun' ~ 6,
                                    month=='Jul' ~ 7,
                                    month=='Aug' ~ 8,
                                    month=='Sep' ~ 9,
                                    month=='Oct' ~ 10,
                                    month=='Nov' ~ 11,
                                    month=='Dec' ~ 12),
           year == as.character(year))%>%
    select(month_number,year, SM_TOTBUS)%>%
    group_by(month_number,year)%>%
    summarize(total_value = sum(SM_TOTBUS))%>%
    ggplot(aes(month_number,total_value, group = year, color = year))+
    geom_line()+
    theme(axis.text.x = element_text(angle = 90, hjust = 1))+
    ylab("Total Value in Millions")+
    scale_x_continuous(name="Month", breaks=c(1:12))+
    ggtitle("Sales per Month")
```
\newpage
A scatterplot was created to see the correlation in monthly sales between the categories of manufacturer, retail, and merchant wholesalers. The retail data has the weakest correlation with the total business sales.

```{r plot8, echo=FALSE}
train%>%filter(!is.na(SM_TOTBUS), 
               !is.na(SM_MNFCTR),
               !is.na(SM_RETAIL),
               !is.na(SM_WHLSLR))%>%
  select(period, SM_TOTBUS, SM_MNFCTR, SM_RETAIL, SM_WHLSLR)%>%
  gather(cat_code, value, c('SM_MNFCTR', 'SM_RETAIL', 'SM_WHLSLR'))%>%
  mutate(category = case_when(cat_code == 'SM_MNFCTR' ~ 'Manufacturer', 
                              cat_code == 'SM_RETAIL' ~ 'Retail', 
                              cat_code == 'SM_WHLSLR' ~ 'Wholesalers'))%>%
  ggplot(aes(x=value,y=SM_TOTBUS, color = category))+
  geom_point()+
  geom_smooth(method = 'loess', formula = y ~x, color = "black")+
  ylab("Total Business Sales")+
  xlab("Sales Values")+
  theme_bw()+
  ggtitle("Total Business Sales vs Other Categories")+
  facet_grid(.~category, scales = "free_x")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "none")
```
\newpage
The final plots in the EDA were created to explore the percent change values for monthly sales. The percent change values represent the percentage that the current month's sales have increased or decreased from the previous month. First, a histogram was created to see the distribution the of the percent change values. Then, a second histogram was created to explore the duration of increasing and decreasing percentage changes.

```{r plot9, echo=FALSE}
train%>%filter(!is.na(MPCSM_TOTBUS))%>%
  mutate(value = round(MPCSM_TOTBUS))%>%
  ggplot(aes(value, fill = value >= 0))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 75, hjust = 1))+
  theme_bw()+
  xlab("Percent Change")+
  ylab("Number of Months")+
  ggtitle("Distribution of Percent Changes for Monthly Sales")
```

```{r plot10, echo=FALSE}
declines<-train%>%
  mutate(decline := MPCSM_TOTBUS < 0)

decline_rle <-rle(declines$decline)
decline_length <-decline_rle$lengths

decline_length<- as.data.frame(decline_length)%>%
  mutate(decline_values = decline_rle$values)%>%
  mutate(type = case_when(decline_values == 'FALSE' ~ "Increase",
                          TRUE ~ "Decrease"))

as.data.frame(decline_length)%>%
  ggplot(aes(decline_length, fill = type))+
  geom_bar(position = "dodge")+
  xlab("Number of Months")+
  ylab("Count")+
  theme_bw()+
  ggtitle("Duration of Increasing and Decreasing Monthly Sales")
```

#### Machine Learning Models

After completing the EDA, the next step was to apply machine learning models to the training data. The EDA revealed an overall upward trend in sales over time. This indicated that month and year were strong factors for predicting the total business monthly sales. In the real world, it is only possible to look at data from previous months when predicting sales for the current month. Therefore, all factors were removed from the dataframe with the exception on the period (month and year) and the previous month's values for monthly sales and monthly percent changes.

```{r final_df,include=FALSE}
train<-train%>%
  select(SM_TOTBUS,per_idx,PREV_SM_TOTBUS, PREV_SM_MNFCTR, PREV_SM_RETAIL, PREV_SM_WHLSLR, 
         PREV_MPCSM_TOTBUS, PREV_MPCSM_MNFCTR, PREV_MPCSM_RETAIL, PREV_MPCSM_WHLSLR)

test<-test%>%
  select(SM_TOTBUS,per_idx,PREV_SM_TOTBUS, PREV_SM_MNFCTR, PREV_SM_RETAIL, PREV_SM_WHLSLR, 
         PREV_MPCSM_TOTBUS, PREV_MPCSM_MNFCTR, PREV_MPCSM_RETAIL, PREV_MPCSM_WHLSLR)

validation<-validation%>%
  select(SM_TOTBUS,per_idx,PREV_SM_TOTBUS, PREV_SM_MNFCTR, PREV_SM_RETAIL, PREV_SM_WHLSLR, 
         PREV_MPCSM_TOTBUS, PREV_MPCSM_MNFCTR, PREV_MPCSM_RETAIL, PREV_MPCSM_WHLSLR)
```

First, a linear model was fit to the training data and predicted the total business monthly sales based on the period. The model was then applied to the test dataset to created predictions.

```{r linear_fit}
#fit a linear model to determine the affect of time on the total monthly sale
fit_lm <- lm(SM_TOTBUS ~ per_idx, data = train)
fit_lm$coefficients

#predict the total monthly sales based on the time period
y_hat_lm <- predict(fit_lm, test)
```

The predictions were evaluated by calculating the Root Mean Square Error (RMSE) between the predicted and actual values for the total monthly sales. A scatterplot was also created to visualize the relationship between the predictions and actual values.

```{r RMSE}
#RMSE calculation to evaluate predictions
RMSE <- function(true_ratings, predicted_ratings,n){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

time_RMSE<-RMSE(test$SM_TOTBUS,y_hat_lm)
```

```{r lm_plot, echo=FALSE}
#plot Predictions versus actual monthly sales
as.data.frame(y_hat_lm)%>%
  mutate(Predictions = y_hat_lm,
         ActualSales = test$SM_TOTBUS)%>%
  ggplot(aes(Predictions,ActualSales))+
  geom_point()+
  xlab("Predicted Sales")+
  ylab("Actual Sales")+
  theme_bw()+
  ggtitle("Linear Model Results")
```

A second model was created using the K-Nearest Neighbors method in order to get more accurate predictions. The K-Nearest Neighbors method compares 'k' number of records with similar values (i.e. closest distance) to create the predictions. This results in a more flexible prediction than a linear model. Initially, 'k' was set to 5. 

```{r knn_fit}
#Fit the knn model on the train data and evaluate on the test data
fit_knn<-knn.reg(train = train, test = test, y = train$SM_TOTBUS, k = 5)

#Evaluate the RMSE
knn_RMSE<-RMSE(test$SM_TOTBUS,fit_knn$pred)
```

```{r knn_plot, echo = FALSE}
#plot Predictions versus actual monthly sales
as.data.frame(fit_knn$pred)%>%
  mutate(Predictions = fit_knn$pred,
         ActualSales = test$SM_TOTBUS)%>%
  ggplot(aes(Predictions,ActualSales))+
  geom_point()+
  xlab("Predicted Sales")+
  ylab("Actual Sales")+
  theme_bw()+
  ggtitle("KNN Model Results")
```

A range of 'k' values from 3 - 100 was evaluated to make sure the optimal value of k is being used. This showed that 3 had the lowest RMSE value, however 'k' was set to 5 in the final model to prevent over-fitting.

```{r num_k}
#Evaluate a range of number of neighbors (K)
ks <- seq(3, 100, 2) #k sequence to test: odd numbers between 3-100
RMSE_results <- map_df(ks, function(k){ 
  fit <- knn.reg(train = train, test = test, y = train$SM_TOTBUS, k = k) #fit the knn value
  test_RMSE<-RMSE(test$SM_TOTBUS,fit$pred)
  tibble(k = k, RMSE = test_RMSE) #create a tibble with all of the train and test accuracy values
})

as.data.frame(RMSE_results)%>%filter(RMSE == min(RMSE))
```

Finally, the KNN model with k = 5 was used to create predictions on the validation dataset and was evaluated by calculating the RMSE.

```{r final_knn}
#Apply knn model to the validation set
fit_knn_val<-knn.reg(train = train, test = validation, y = train$SM_TOTBUS, k = 5)

#Evaluate the RMSE
knn_RMSE_val<-RMSE(validation$SM_TOTBUS,fit_knn_val$pred)
```

## Results

The linear model predicted the total business monthly sales based solely on the time period and was able to create predictions with a RMSE value of `r time_RMSE` million USD. This is a decently small RMSE given that the average monthly sales value is `r avg_sales` million USD. However, a more accurate model was achieved using the KNN method which considered the previous period's values in addition to the date. 

The final KNN model had a RMSE equal to  `r knn_RMSE_val` million USD. The below scatterplot shows the correlation between the predictions and the actual values.

``` {r final_plot, echo = FALSE}
as.data.frame(fit_knn_val$pred)%>%
  mutate(Predictions = fit_knn_val$pred,
         ActualSales = validation$SM_TOTBUS)%>%
  ggplot(aes(Predictions,ActualSales))+
  geom_point()+
  xlab("Predicted Sales")+
  ylab("Actual Sales")+
  theme_bw()+
  ggtitle("Validation Data: KNN Model Results")
```

## Conclusion

The KNN model returned an reasonable RMSE value of `r knn_RMSE_val`. The solution was created from the exploratory data analysis. The factors for time period, previous period's monthly sales and previous period's percent changes were major contributors to the low RMSE score. The largest limitation to this project was the relatively small dataset. Additionally, the model was creating predictions solely based on monetary factors and did not consider any political or global events. In reality, predicting sales one month in advance is much simplier than a quarter or year in advance. Future work could be done to predict monthly sales for different categories and for longer date ranges. Other machine learning models could be applied and potentially provide a lower RMSE.